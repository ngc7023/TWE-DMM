import osimport jiebaimport codecsimport _pickle as cPickle# 从文件导入停用词表stpwrdpath = "../data/classifydata/stop/stopword.txt"stpwrd_dic = codecs.open(stpwrdpath, encoding = 'utf-8')stpwrd_content = stpwrd_dic.read()# 将停用词表转换为liststpwrdlst = stpwrd_content.splitlines()stpwrd_dic.close()# print(stpwrdlst)def segText(inputPath, resultfile):	doc_list=[]	label_list=[]	fatherLists = os.listdir(inputPath)  # 主目录	for eachDir in fatherLists:  # 遍历主目录中各个文件夹		if eachDir[0]=='.':			continue		eachPath = inputPath + eachDir + "/"  # 保存主目录中每个文件夹目录，便于遍历二级文件		childLists = os.listdir(eachPath)  # 获取每个文件夹中的各个文件		for eachFile in childLists:  # 遍历每个文件夹中的子文件			eachPathFile = eachPath + eachFile  # 获得每个文件路径			f=codecs.open(eachPathFile,encoding = 'GBK')			# print(eachPathFile)			text_string=''			for line in f:				text_string+=line.strip()			token_list=[w for w in jieba.cut(text_string) if w not in stpwrdlst and w not in [' ','']]			if len(token_list)==0:				print('token_list len=0')			doc_list.append(token_list)			label_list.append(eachPathFile)	print(len(doc_list),len(label_list))	cPickle.dump([doc_list,label_list], open(resultfile, "wb"))def createtextcorpus(datafile,outputfile):	x=cPickle.load(open(datafile,'rb'))	doc_list=x[0]	label_list=x[1]	del x	f=codecs.open(outputfile,'w',encoding = 'utf-8')	for d in doc_list:		if len(d)==0:			print('len=0')		f.write(' '.join(d)+'\n')	f.close()	print(len(doc_list),len(label_list))def merge_corpus(traindata,testdata,outfile1,outfile2):	x=cPickle.load(open(traindata,'rb'))	y=cPickle.load(open(testdata,'rb'))	doc_list=x[0]+y[0]	label_list=x[1]+y[1]	print(len(doc_list),len(label_list))	cPickle.dump([doc_list,label_list],open(outfile1,'wb'))	f = codecs.open(outfile2, 'w', encoding = 'utf-8')	for d in doc_list:		if len(d) == 0:			print('len=0')		f.write(' '.join(d) + '\n')	f.close()def precess1(datafile,outfile1,outfile2):	x=cPickle.load(open(datafile,'rb'))	doc_list=x[0]	label_list=x[1]	del x	vocab={}	for doc in doc_list:		for w in doc:			if w in vocab:				vocab[w]+=1			else:				vocab[w]=1	print(len(vocab))	vocab1={}	for w in vocab:		if vocab[w]>2:			vocab1[w]=vocab[w]	print('len_vocab:',len(vocab1))	wordtoix = {}	ixtoword = {}	wordtoix['UNK'] = 0	ixtoword[0] = 'UNK'	ix = 1	for w in vocab1:		wordtoix[w] = ix		ixtoword[ix] = w		ix += 1	doccententindex = []	ouf2=codecs.open(outfile2,'w')	max_len=0	for doc in doc_list:		docix_list = []		doctext=''		for w in doc:			if w in wordtoix:				docix_list.append(wordtoix[w])				doctext+=w+' '			else:				docix_list.append(wordtoix['UNK'])				doctext+='UNK'+' '		if max_len<len(docix_list):			max_len=len(docix_list)		doccententindex.append(docix_list)		ouf2.write(doctext.strip()+'\n')	ouf2.close()	print(len(doccententindex),len(label_list))	print('max_len:',max_len)	cPickle.dump([doccententindex,label_list,wordtoix, ixtoword], open(outfile1, "wb"))def test(datafile,embfile):	vocab=[]	f=codecs.open(embfile,'r')	for line in f:		vocab.append(line.strip().split()[0])	f.close()	f2=codecs.open(datafile,'r')	for line in f2:		tokens=line.split()		for t in tokens:			if t not in vocab:				print(t)	f2.close()if __name__=='__main__':	# 分词，第一个是分词输入，第二个参数是结果保存的路径	segText("../data/classifydata/data/", "../data/classifydata/classify_train_corpus.p")	segText("../data/classifydata/test1/", "../data/classifydata/classify_test_corpus.p")	createtextcorpus('../data/classifydata/classify_train_corpus.p','../data/classifydata/classify_train.txt')	createtextcorpus('../data/classifydata/classify_test_corpus.p', '../data/classifydata/classify_test.txt')	traindata='../data/classifydata/classify_train_corpus.p'	testdata='../data/classifydata/classify_test_corpus.p'	outfile1='../data/classifydata/classifydata.p'	outfile2='../data/classifydata/classify_corpus.txt'	merge_corpus(traindata,testdata,outfile1,outfile2)	datafile='../data/classifydata/classifydata.p'	outfile1='../data/classifydata/classifydata_index.p'	outfile2='../data/classifydata/classify_corpus_less2.txt'	precess1(datafile,outfile1,outfile2)	test('../data/classifydata/classify_corpus_less2.txt','../data/classifydata/classifydata_emb_LFTM.txt')